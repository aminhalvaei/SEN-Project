{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0032a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a471a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(filepath: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse a single log file to extract decay rate, training losses over steps, and final metrics.\n",
    "    \"\"\"\n",
    "    # Extract decay rate from filename\n",
    "    filename = os.path.basename(filepath)\n",
    "    decay_match = re.search(r'run_param_([\\d.]+|0)\\.log', filename)\n",
    "    if not decay_match:\n",
    "        print(f\"Warning: Could not extract decay rate from filename {filename}. Skipping.\")\n",
    "        return None\n",
    "    decay_rate = float(decay_match.group(1)) if decay_match.group(1) != '0' else 0.0\n",
    "    \n",
    "    losses = []  # List of (step, pos_loss, neg_loss, avg_loss, temp)\n",
    "    valid_metrics = {}\n",
    "    test_metrics = {}\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        # Extract training losses at steps\n",
    "        step_match = re.search(r'Training average positive_sample_loss at step (\\d+): ([\\d.]+)', line)\n",
    "        if step_match:\n",
    "            step = int(step_match.group(1))\n",
    "            pos_loss = float(step_match.group(2))\n",
    "            losses.append({'step': step, 'pos_loss': pos_loss})\n",
    "            continue\n",
    "        \n",
    "        neg_match = re.search(r'Training average negative_sample_loss at step (\\d+): ([\\d.]+)', line)\n",
    "        if neg_match:\n",
    "            step = int(neg_match.group(1))\n",
    "            neg_loss = float(neg_match.group(2))\n",
    "            for loss_dict in losses:\n",
    "                if loss_dict['step'] == step:\n",
    "                    loss_dict['neg_loss'] = neg_loss\n",
    "                    break\n",
    "        \n",
    "        avg_match = re.search(r'Training average loss at step (\\d+): ([\\d.]+)', line)\n",
    "        if avg_match:\n",
    "            step = int(avg_match.group(1))\n",
    "            avg_loss = float(avg_match.group(2))\n",
    "            for loss_dict in losses:\n",
    "                if loss_dict['step'] == step:\n",
    "                    loss_dict['avg_loss'] = avg_loss\n",
    "                    break\n",
    "        \n",
    "        temp_match = re.search(r'Training average current_temperature at step (\\d+): ([\\d.]+)', line)\n",
    "        if temp_match:\n",
    "            step = int(temp_match.group(1))\n",
    "            temp = float(temp_match.group(2))\n",
    "            for loss_dict in losses:\n",
    "                if loss_dict['step'] == step:\n",
    "                    loss_dict['temp'] = temp\n",
    "                    break\n",
    "        \n",
    "        # Extract valid metrics at step 4999\n",
    "        valid_mrr = re.search(r'Valid MRR at step 4999: ([\\d.]+)', line)\n",
    "        if valid_mrr:\n",
    "            valid_metrics['mrr'] = float(valid_mrr.group(1))\n",
    "        valid_mr = re.search(r'Valid MR at step 4999: ([\\d.]+)', line)\n",
    "        if valid_mr:\n",
    "            valid_metrics['mr'] = float(valid_mr.group(1))\n",
    "        valid_hits1 = re.search(r'Valid HITS@1 at step 4999: ([\\d.]+)', line)\n",
    "        if valid_hits1:\n",
    "            valid_metrics['hits1'] = float(valid_hits1.group(1))\n",
    "        valid_hits3 = re.search(r'Valid HITS@3 at step 4999: ([\\d.]+)', line)\n",
    "        if valid_hits3:\n",
    "            valid_metrics['hits3'] = float(valid_hits3.group(1))\n",
    "        valid_hits10 = re.search(r'Valid HITS@10 at step 4999: ([\\d.]+)', line)\n",
    "        if valid_hits10:\n",
    "            valid_metrics['hits10'] = float(valid_hits10.group(1))\n",
    "        \n",
    "        # Extract test metrics at step 4999\n",
    "        test_mrr = re.search(r'Test MRR at step 4999: ([\\d.]+)', line)\n",
    "        if test_mrr:\n",
    "            test_metrics['mrr'] = float(test_mrr.group(1))\n",
    "        test_mr = re.search(r'Test MR at step 4999: ([\\d.]+)', line)\n",
    "        if test_mr:\n",
    "            test_metrics['mr'] = float(test_mr.group(1))\n",
    "        test_hits1 = re.search(r'Test HITS@1 at step 4999: ([\\d.]+)', line)\n",
    "        if test_hits1:\n",
    "            test_metrics['hits1'] = float(test_hits1.group(1))\n",
    "        test_hits3 = re.search(r'Test HITS@3 at step 4999: ([\\d.]+)', line)\n",
    "        if test_hits3:\n",
    "            test_metrics['hits3'] = float(test_hits3.group(1))\n",
    "        test_hits10 = re.search(r'Test HITS@10 at step 4999: ([\\d.]+)', line)\n",
    "        if test_hits10:\n",
    "            test_metrics['hits10'] = float(test_hits10.group(1))\n",
    "    \n",
    "    # Ensure we have valid data before returning\n",
    "    if not test_metrics:\n",
    "        print(f\"Warning: No test metrics found in {filename}. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'decay_rate': decay_rate,\n",
    "        'losses': pd.DataFrame(losses),\n",
    "        'valid_metrics': valid_metrics,\n",
    "        'test_metrics': test_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5199ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(log_files: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Collect data from all log files, filtering out None results.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for file in log_files:\n",
    "        parsed = parse_log_file(file)\n",
    "        if parsed:\n",
    "            data.append(parsed)\n",
    "        else:\n",
    "            print(f\"Skipping file {file} due to parsing issues.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccb7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(data: List[Dict], output_dir: str = 'visualizations'):\n",
    "    \"\"\"\n",
    "    Create recommended visualizations and save them as PDF.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare DataFrame for final metrics\n",
    "    metrics_data = []\n",
    "    for d in data:\n",
    "        if d['test_metrics']:  # Only include entries with test metrics\n",
    "            metrics_data.append({\n",
    "                'decay_rate': d['decay_rate'],\n",
    "                'test_mrr': d['test_metrics'].get('mrr', None),\n",
    "                'test_mr': d['test_metrics'].get('mr', None),\n",
    "                'test_hits1': d['test_metrics'].get('hits1', None),\n",
    "                'test_hits3': d['test_metrics'].get('hits3', None),\n",
    "                'test_hits10': d['test_metrics'].get('hits10', None),\n",
    "                'valid_mrr': d['valid_metrics'].get('mrr', None),\n",
    "            })\n",
    "    \n",
    "    if not metrics_data:\n",
    "        raise ValueError(\"No valid metrics data found to create visualizations.\")\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    print(\"Final Metrics Table:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    # Sort by decay_rate, ensuring it exists\n",
    "    if 'decay_rate' not in metrics_df.columns:\n",
    "        raise KeyError(\"Column 'decay_rate' not found in metrics DataFrame.\")\n",
    "    metrics_df = metrics_df.sort_values('decay_rate')\n",
    "    \n",
    "    # 1. Line Plot: Metrics vs Decay Rate\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='decay_rate', y='test_mrr', data=metrics_df, marker='o', label='Test MRR')\n",
    "    sns.lineplot(x='decay_rate', y='test_hits1', data=metrics_df, marker='o', label='Test HITS@1')\n",
    "    sns.lineplot(x='decay_rate', y='test_hits3', data=metrics_df, marker='o', label='Test HITS@3')\n",
    "    sns.lineplot(x='decay_rate', y='test_hits10', data=metrics_df, marker='o', label='Test HITS@10')\n",
    "    plt.xlabel('Decay Rate')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Test Metrics vs Decay Rate')\n",
    "    plt.xscale('log')  # Log scale for better visualization of small decay rates\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_vs_decay_line.pdf'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Bar Chart: Clustered bars for metrics per decay rate\n",
    "    melted_df = pd.melt(metrics_df, id_vars=['decay_rate'], value_vars=['test_mrr', 'test_hits1', 'test_hits3', 'test_hits10'])\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='decay_rate', y='value', hue='variable', data=melted_df)\n",
    "    plt.xlabel('Decay Rate')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Test Metrics Across Decay Rates')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_bar_chart.pdf'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Heatmap: Metrics across decay rates\n",
    "    heatmap_df = metrics_df.set_index('decay_rate')[['test_mrr', 'test_hits1', 'test_hits3', 'test_hits10']]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.4f')\n",
    "    plt.title('Heatmap of Test Metrics vs Decay Rate')\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_heatmap.pdf'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Loss Curves: Average Loss vs Step for each decay rate\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for d in data:\n",
    "        losses_df = d['losses']\n",
    "        if 'avg_loss' in losses_df.columns:\n",
    "            sns.lineplot(x='step', y='avg_loss', data=losses_df, label=f'Decay: {d[\"decay_rate\"]:.6f}')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Average Training Loss vs Step Across Decay Rates')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.savefig(os.path.join(output_dir, 'loss_vs_step.pdf'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Temperature Decay: Current Temperature vs Step\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for d in data:\n",
    "        losses_df = d['losses']\n",
    "        if 'temp' in losses_df.columns:\n",
    "            sns.lineplot(x='step', y='temp', data=losses_df, label=f'Decay: {d[\"decay_rate\"]:.6f}')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Current Temperature')\n",
    "    plt.title('Adversarial Temperature vs Step Across Decay Rates')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.savefig(os.path.join(output_dir, 'temperature_vs_step.pdf'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Scatter Plot: Trade-off between MRR and MR\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x='test_mrr', y='test_mr', data=metrics_df, hue='decay_rate', palette='viridis', size='decay_rate', sizes=(50, 300))\n",
    "    plt.xlabel('Test MRR')\n",
    "    plt.ylabel('Test MR')\n",
    "    plt.title('Trade-off: Test MRR vs MR (Sized by Decay Rate)')\n",
    "    plt.savefig(os.path.join(output_dir, 'mrr_vs_mr_scatter.pdf'), bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286d5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 log files: ['logs\\\\run_param_0.0001.log', 'logs\\\\run_param_0.00025.log', 'logs\\\\run_param_0.0005.log', 'logs\\\\run_param_0.00075.log', 'logs\\\\run_param_0.001.log', 'logs\\\\run_param_0.0025.log', 'logs\\\\run_param_0.005.log', 'logs\\\\run_param_0.0075.log', 'logs\\\\run_param_0.log']\n",
      "Final Metrics Table:\n",
      "   decay_rate  test_mrr      test_mr  test_hits1  test_hits3  test_hits10  \\\n",
      "0     0.00010  0.249737   771.457124    0.177147    0.274382     0.390648   \n",
      "1     0.00025  0.242231   823.963794    0.170087    0.268176     0.382732   \n",
      "2     0.00050  0.232334   900.238420    0.162220    0.255912     0.368269   \n",
      "3     0.00075  0.230468   929.886812    0.160974    0.253665     0.366852   \n",
      "4     0.00100  0.226376   961.894288    0.156919    0.249707     0.362650   \n",
      "5     0.00250  0.224012   998.459323    0.155771    0.247826     0.354857   \n",
      "6     0.00500  0.222537   989.006523    0.153670    0.245431     0.356420   \n",
      "7     0.00750  0.221864  1004.425169    0.152961    0.246433     0.356420   \n",
      "8     0.00000  0.256839   724.110525    0.182058    0.283666     0.402961   \n",
      "\n",
      "   valid_mrr  \n",
      "0   0.252854  \n",
      "1   0.245774  \n",
      "2   0.236095  \n",
      "3   0.233488  \n",
      "4   0.230149  \n",
      "5   0.225893  \n",
      "6   0.226018  \n",
      "7   0.225487  \n",
      "8   0.260894  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    log_files = glob.glob('logs/run_param_*.log')  # Adjust path if needed\n",
    "    if not log_files:\n",
    "        raise FileNotFoundError(\"No log files found matching 'run_param_*.log'. Check the directory.\")\n",
    "    print(f\"Found {len(log_files)} log files: {log_files}\")\n",
    "    data = collect_data(log_files)\n",
    "    if not data:\n",
    "        raise ValueError(\"No valid data parsed from log files.\")\n",
    "    create_visualizations(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465f8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rotate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
